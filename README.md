# Docker-ETL-Pipeline
An extract, transform load data pipeline composed with Docker. The docker-compose yaml file defines the source database, destination database and python script as services for the pipeline. When run by the Dockerfile, the python script etl_script.py establishes a connection to the source database. It then reads the Users and Authors tables into dataframes, upon which various transformations are made. The name fields of the Users table are standardized to not include any additional text after whitespaces, and records with defunct email domains are dropped. The Authors table comes with redundant data due to a functional dependency between city_of_origin and country_of_origin, so the table is normalized by way of splitting the location information off into a separate Locations table. The pipeline then loads the transformed data to the destination database and executes an SQL statement which defines the primary keys for each of the three table. The resulting data can be viewed by executing the command docker exec -it etl-destination_postgres-1 psql -U postgres and connecting to the database "destination_db".